#!/usr/bin/env python
# coding: utf-8

# # GÃ©nÃ©ration de prÃ©dictions pour tous les fichiers d'images

import os
import torch
import torch.nn as nn
import nibabel as nib
import numpy as np
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
from datetime import datetime
from pathlib import Path

# Setup device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Device utilisÃ©: {device}")

# Chemin vers le modÃ¨le prÃ©-entraÃ®nÃ©
MODEL_PATH = "best_model_gridsearch_20250606_223151.pth"  # Modifiez si nÃ©cessaire

# ## ModÃ¨le U-Net (copie depuis train_from_checkpoint.py)

class DoubleConv(nn.Module):
    """Deux convolutions + ReLU + BatchNorm"""
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.double_conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        return self.double_conv(x)

class UNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.down1 = DoubleConv(1, 32)
        self.pool1 = nn.MaxPool2d(2)
        self.down2 = DoubleConv(32, 64)
        self.pool2 = nn.MaxPool2d(2)

        self.bottleneck = DoubleConv(64, 128)

        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)
        self.upconv1 = DoubleConv(128, 64)
        self.up2 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)
        self.upconv2 = DoubleConv(64, 32)

        self.final_conv = nn.Conv2d(32, 1, kernel_size=1)  # binaire

    def forward(self, x):
        d1 = self.down1(x)
        d2 = self.down2(self.pool1(d1))
        b = self.bottleneck(self.pool2(d2))

        u1 = self.up1(b)
        u1 = self.upconv1(torch.cat([u1, d2], dim=1))
        u2 = self.up2(u1)
        u2 = self.upconv2(torch.cat([u2, d1], dim=1))

        out = self.final_conv(u2)
        return torch.sigmoid(out)  # probas entre 0 et 1

# ## Dataset pour prÃ©diction uniquement (sans labels)

class InferenceDataset(Dataset):
    def __init__(self, images_dir, slice_axis=2):
        self.images_dir = images_dir
        self.filenames = sorted([f for f in os.listdir(images_dir) if f.endswith('.nii.gz')])
        self.slice_axis = slice_axis
        print(f"ğŸ“ TrouvÃ© {len(self.filenames)} fichiers d'images")

    def __len__(self):
        return len(self.filenames)

    def __getitem__(self, idx):
        img_path = os.path.join(self.images_dir, self.filenames[idx])
        filename = self.filenames[idx]
        
        # Charger l'image 3D
        image = nib.load(img_path).get_fdata()
        
        # Obtenir les dimensions originales pour reconstruction
        original_shape = image.shape
        
        # Extraire toutes les slices selon l'axe spÃ©cifiÃ©
        slices = []
        if self.slice_axis == 0:
            for i in range(image.shape[0]):
                slice_2d = image[i, :, :]
                slices.append(slice_2d)
        elif self.slice_axis == 1:
            for i in range(image.shape[1]):
                slice_2d = image[:, i, :]
                slices.append(slice_2d)
        else:  # slice_axis == 2
            for i in range(image.shape[2]):
                slice_2d = image[:, :, i]
                slices.append(slice_2d)
        
        # Normaliser et convertir en tenseurs
        processed_slices = []
        for slice_2d in slices:
            # Normalisation
            if np.max(slice_2d) > np.min(slice_2d):
                slice_normalized = (slice_2d - np.min(slice_2d)) / (np.max(slice_2d) - np.min(slice_2d))
            else:
                slice_normalized = slice_2d
            
            # Convertir en tensor [1, H, W]
            slice_tensor = torch.tensor(slice_normalized, dtype=torch.float32).unsqueeze(0)
            processed_slices.append(slice_tensor)
        
        return processed_slices, filename, original_shape

# ## Fonction pour charger le modÃ¨le

def load_pretrained_model(model_path, device):
    """Charge un modÃ¨le prÃ©-entraÃ®nÃ© depuis un fichier .pth"""
    if not os.path.exists(model_path):
        print(f"âŒ Erreur: Le fichier {model_path} n'existe pas!")
        return None
    
    try:
        model = UNet().to(device)
        checkpoint = torch.load(model_path, map_location=device)
        model.load_state_dict(checkpoint)
        model.eval()  # Mode Ã©valuation
        print(f"âœ… ModÃ¨le chargÃ© avec succÃ¨s depuis: {model_path}")
        return model
    except Exception as e:
        print(f"âŒ Erreur lors du chargement du modÃ¨le: {e}")
        return None

# ## Fonction de calcul du Dice Score

def calculate_dice_score(pred, target, smooth=1e-6):
    """Calcule le score Dice entre prÃ©diction et vÃ©ritÃ© terrain"""
    pred = (pred > 0.5).astype(np.float32)
    target = target.astype(np.float32)
    
    intersection = (pred * target).sum()
    union = pred.sum() + target.sum()
    
    if union == 0:
        return 1.0 if intersection == 0 else 0.0
    
    dice = (2. * intersection + smooth) / (union + smooth)
    return dice

# ## Fonction principale de gÃ©nÃ©ration des prÃ©dictions

def generate_all_predictions(images_dir, output_dir, model_path, device, threshold=0.5, labels_dir=None):
    """
    GÃ©nÃ¨re des prÃ©dictions pour tous les fichiers du dossier images
    et les sauvegarde dans le dossier de sortie avec les mÃªmes noms
    Si labels_dir est fourni, calcule aussi le score Dice de validation
    """
    
    print(f"ğŸš€ GÃ‰NÃ‰RATION DES PRÃ‰DICTIONS")
    print(f"Dossier d'images: {images_dir}")
    print(f"Dossier de sortie: {output_dir}")
    print(f"ModÃ¨le: {model_path}")
    if labels_dir:
        print(f"Dossier de validation: {labels_dir}")
    print("-" * 60)
    
    # VÃ©rifier que le dossier d'images existe
    if not os.path.exists(images_dir):
        print(f"âŒ Le dossier d'images {images_dir} n'existe pas!")
        return False
    
    # CrÃ©er le dossier de sortie
    os.makedirs(output_dir, exist_ok=True)
    print(f"ğŸ“ Dossier de sortie crÃ©Ã©: {output_dir}")
    
    # Charger le modÃ¨le
    model = load_pretrained_model(model_path, device)
    if model is None:
        return False
    
    # CrÃ©er le dataset
    dataset = InferenceDataset(images_dir)
    if len(dataset) == 0:
        print("âŒ Aucun fichier .nii.gz trouvÃ© dans le dossier d'images!")
        return False
      # Traiter chaque volume
    print(f"\nğŸ”„ Traitement de {len(dataset)} volumes...")
    
    dice_scores = []
    validation_available = labels_dir is not None and os.path.exists(labels_dir)
    
    if validation_available:
        print("ğŸ“Š Calcul du score Dice de validation activÃ©")
    
    with torch.no_grad():
        for idx in tqdm(range(len(dataset)), desc="GÃ©nÃ©ration des prÃ©dictions"):
            slices, filename, original_shape = dataset[idx]
            
            # Charger le label de vÃ©ritÃ© terrain si disponible
            ground_truth = None
            if validation_available:
                label_path = os.path.join(labels_dir, filename)
                if os.path.exists(label_path):
                    try:
                        ground_truth = nib.load(label_path).get_fdata()
                    except Exception as e:
                        print(f"âš ï¸ Erreur chargement label {filename}: {e}")
            
            # Traiter toutes les slices du volume
            predicted_slices = []
            
            for slice_tensor in slices:
                # Ajouter une dimension batch [1, 1, H, W]
                slice_batch = slice_tensor.unsqueeze(0).to(device)
                
                # PrÃ©diction
                pred = model(slice_batch)
                
                # Appliquer le seuil et convertir en uint8
                pred_binary = (pred.squeeze().cpu().numpy() > threshold).astype(np.uint8)
                predicted_slices.append(pred_binary)
            
            # Reconstruire le volume 3D selon l'axe original
            if dataset.slice_axis == 0:
                prediction_volume = np.stack(predicted_slices, axis=0)
            elif dataset.slice_axis == 1:
                prediction_volume = np.stack(predicted_slices, axis=1)
            else:  # slice_axis == 2
                prediction_volume = np.stack(predicted_slices, axis=2)
            
            # Calculer le score Dice si ground truth disponible
            dice_score = None
            if ground_truth is not None:
                try:
                    dice_score = calculate_dice_score(prediction_volume, ground_truth)
                    dice_scores.append(dice_score)
                except Exception as e:
                    print(f"âš ï¸ Erreur calcul Dice pour {filename}: {e}")
            
            # VÃ©rifier que les dimensions correspondent
            if prediction_volume.shape != original_shape:
                print(f"âš ï¸ Attention: Dimensions diffÃ©rentes pour {filename}")
                print(f"   Original: {original_shape}, PrÃ©diction: {prediction_volume.shape}")
            
            # Sauvegarder en format .nii.gz
            nii_img = nib.Nifti1Image(prediction_volume, affine=np.eye(4))
            output_path = os.path.join(output_dir, filename)
            nib.save(nii_img, output_path)
            
            # Afficher le rÃ©sultat avec Dice si disponible
            if dice_score is not None:
                print(f"âœ… SauvegardÃ©: {filename} - Shape: {prediction_volume.shape} - Dice: {dice_score:.4f}")
            else:
                print(f"âœ… SauvegardÃ©: {filename} - Shape: {prediction_volume.shape}")
    
    print(f"\nğŸ‰ GÃ‰NÃ‰RATION TERMINÃ‰E!")
    print(f"ğŸ“ {len(dataset)} prÃ©dictions sauvegardÃ©es dans: {output_dir}")
    
    # Afficher les statistiques Dice si disponibles
    if dice_scores:
        avg_dice = np.mean(dice_scores)
        std_dice = np.std(dice_scores)
        min_dice = np.min(dice_scores)
        max_dice = np.max(dice_scores)
        
        print(f"\nğŸ“Š STATISTIQUES DE VALIDATION:")
        print(f"Nombre de volumes validÃ©s: {len(dice_scores)}")
        print(f"Score Dice moyen: {avg_dice:.4f} (Â±{std_dice:.4f})")
        print(f"Score Dice min: {min_dice:.4f}")
        print(f"Score Dice max: {max_dice:.4f}")
        
        # Classification de performance
        if avg_dice >= 0.8:
            performance = "ğŸŸ¢ EXCELLENTE"
        elif avg_dice >= 0.7:
            performance = "ğŸŸ¡ BONNE"
        elif avg_dice >= 0.6:
            performance = "ğŸŸ  MOYENNE"
        else:
            performance = "ğŸ”´ FAIBLE"
        
        print(f"Performance globale: {performance}")
    
    return True

# ## Fonction pour vÃ©rifier la structure des dossiers

def setup_directories():
    """VÃ©rifie et crÃ©e la structure de dossiers nÃ©cessaire"""
    
    # Chemins des dossiers
    dataset_dir = "./final_dataset"
    images_dir = "./final_dataset/images"
    labels_dir = "./final_dataset/labels"
    validation_labels_dir = "./final_dataset/labels"  # Labels de validation (existants)
    
    print("ğŸ” VÃ©rification de la structure des dossiers...")
    
    # VÃ©rifier si le dossier DatasetChallenge existe
    if not os.path.exists(dataset_dir):
        print(f"âŒ Le dossier {dataset_dir} n'existe pas!")
        print("ğŸ“¦ Peut-Ãªtre devez-vous extraire les fichiers ZIP d'abord?")
        
        # Lister les fichiers ZIP disponibles
        zip_files = [f for f in os.listdir(".") if f.endswith(".zip")]
        if zip_files:
            print("ğŸ“ Fichiers ZIP trouvÃ©s:")
            for zip_file in zip_files:
                print(f"  - {zip_file}")
            print("ğŸ’¡ Conseil: Utilisez unzip_script.py pour extraire les donnÃ©es")
        return None, None, None
    
    # VÃ©rifier le dossier images
    if not os.path.exists(images_dir):
        print(f"âŒ Le dossier {images_dir} n'existe pas!")
        return None, None, None
    
    # CrÃ©er un dossier sÃ©parÃ© pour les nouvelles prÃ©dictions
    new_predictions_dir = "./final_dataset/predictions_generated"
    if not os.path.exists(new_predictions_dir):
        print(f"ğŸ“ CrÃ©ation du dossier {new_predictions_dir}...")
        os.makedirs(new_predictions_dir, exist_ok=True)
    
    # Compter les fichiers
    image_files = [f for f in os.listdir(images_dir) if f.endswith('.nii.gz')]
    print(f"âœ… Dossier images: {len(image_files)} fichiers .nii.gz")
    
    # VÃ©rifier si les labels de validation existent
    if os.path.exists(validation_labels_dir):
        label_files = [f for f in os.listdir(validation_labels_dir) if f.endswith('.nii.gz')]
        print(f"âœ… Labels de validation: {len(label_files)} fichiers .nii.gz")
    else:
        validation_labels_dir = None
        print("âš ï¸ Pas de labels de validation trouvÃ©s")
    
    return images_dir, new_predictions_dir, validation_labels_dir

# ## SCRIPT PRINCIPAL

if __name__ == "__main__":
    print("ğŸ¥ GÃ‰NÃ‰RATEUR DE PRÃ‰DICTIONS POUR SEGMENTATION MÃ‰DICALE")
    print("=" * 60)
    
    # VÃ©rifier et configurer les dossiers
    images_dir, predictions_dir, validation_labels_dir = setup_directories()
    
    if images_dir is None:
        print("âŒ Impossible de continuer sans le dossier d'images!")
        print("ğŸ’¡ Assurez-vous que ./final_dataset/images existe et contient des fichiers .nii.gz")
        exit(1)
    
    # GÃ©nÃ©rer les prÃ©dictions avec validation si possible
    success = generate_all_predictions(
        images_dir=images_dir,
        output_dir=predictions_dir,
        model_path=MODEL_PATH,
        device=device,
        threshold=0.5,
        labels_dir=validation_labels_dir
    )
    
    if success:
        print(f"\nâœ… SUCCÃˆS! Toutes les prÃ©dictions ont Ã©tÃ© gÃ©nÃ©rÃ©es")
        print(f"ğŸ“ VÃ©rifiez le dossier: {predictions_dir}")
        
        # Lister les fichiers crÃ©Ã©s
        created_files = [f for f in os.listdir(predictions_dir) if f.endswith('.nii.gz')]
        print(f"ğŸ“ {len(created_files)} fichiers de labels crÃ©Ã©s:")
        for i, filename in enumerate(sorted(created_files)[:10]):  # Afficher les 10 premiers
            print(f"  {i+1:2d}. {filename}")
        if len(created_files) > 10:
            print(f"     ... et {len(created_files) - 10} autres fichiers")
        
        if validation_labels_dir:
            print(f"\nğŸ’¡ Les scores Dice de validation ont Ã©tÃ© calculÃ©s et affichÃ©s ci-dessus")
        else:
            print(f"\nğŸ’¡ Aucune validation effectuÃ©e (pas de labels de rÃ©fÃ©rence trouvÃ©s)")
    else:
        print("âŒ Ã‰chec de la gÃ©nÃ©ration des prÃ©dictions!")
